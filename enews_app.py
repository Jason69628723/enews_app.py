import streamlit as st
import docx
import re
import base64
from io import BytesIO
from bs4 import BeautifulSoup
import traceback # å¼•å…¥ traceback æ¨¡çµ„

# --- æ¨£å¼è¨­å®š (å¯åœ¨æ­¤èª¿æ•´) ---
STYLE_H1 = "font-size: 28px; font-weight: bold; line-height: 1.8; margin-bottom: 20px;"
STYLE_H2 = "font-size: 22px; font-weight: bold; line-height: 2; margin-top: 25px; margin-bottom: 10px;"
STYLE_H3 = "font-size: 20px; font-weight: bold; line-height: 2; margin-top: 20px; margin-bottom: 10px;"
STYLE_P = "font-size: 20px; line-height: 2; margin-bottom: 15px;"
STYLE_IMG_CONTAINER = "text-align: center; margin-top: 20px; margin-bottom: 20px;"
STYLE_IMG = "max-width: 90%; height: auto; border-radius: 8px;"

# --- æ ¸å¿ƒé‚è¼¯å‡½å¼ ---

def get_heading_level(text, is_first_p):
    """
    ä½¿ç”¨å•Ÿç™¼å¼è¦å‰‡åˆ¤æ–·æ–‡å­—æ®µè½çš„æ¨™é¡Œç´šåˆ¥ã€‚
    """
    stripped_text = text.strip()
    if not stripped_text:
        return None

    if is_first_p:
        return 'h1'

    if re.match(r'^([ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹å]+[ã€ï¼\.])|(\d+\.)|(\([ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹å]+\))|(ç¬¬[ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹å]+[ç« ç¯€])', stripped_text):
        return 'h2'

    if len(stripped_text) < 20 and stripped_text[-1] not in ['ã€‚', 'ï¼Œ', 'ï¼Ÿ', 'ï¼', 'ï¼›', ':', 'ï¼š', ')', 'ï¼‰', '.']:
        return 'h3'

    return 'p'

def generate_meta_description(html_parts):
    """
    å¾HTMLç‰‡æ®µä¸­æ‰¾åˆ°ç¬¬ä¸€å€‹<p>æ¨™ç±¤çš„å…§å®¹ï¼Œç”Ÿæˆmeta descriptionã€‚
    """
    for part in html_parts:
        if part.strip().startswith('<p'):
            soup = BeautifulSoup(part, 'html.parser')
            p_text = soup.get_text().strip()
            meta_content = (p_text[:150] + '...') if len(p_text) > 150 else p_text
            meta_content = meta_content.replace('"', "'")
            return f'<meta name="description" content="{meta_content}">'
    return '<meta name="description" content="ä¸€ç¯‡ç²¾å½©çš„æ–‡ç« å…§å®¹ã€‚">'

def process_docx(file_stream):
    """
    è™•ç†ä¸Šå‚³çš„ .docx æ–‡ä»¶ã€‚
    """
    doc = docx.Document(file_stream)
    html_parts = []
    is_first_paragraph = True

    img_map = {}
    for rel in doc.part.rels.values():
        if "image" in rel.target_ref:
            img_data = rel.target_part.blob
            img_base64 = base64.b64encode(img_data).decode('utf-8')
            img_map[rel.rId] = f"data:{rel.target_part.content_type};base64,{img_base64}"

    for block in doc.element.body:
        if isinstance(block, docx.oxml.text.paragraph.CT_P):
            p = docx.text.paragraph.Paragraph(block, doc)
            
            img_rids = p._p.xpath('.//a:blip/@r:embed')
            if img_rids:
                for rid in img_rids:
                    if rid in img_map:
                        img_src = img_map[rid]
                        html_parts.append(f'<div style="{STYLE_IMG_CONTAINER}"><img src="{img_src}" style="{STYLE_IMG}" alt="æ–‡ç« æ’åœ–"></div>')
            
            text = p.text.strip()
            if text:
                level = get_heading_level(text, is_first_paragraph)
                if level:
                    style_map = {'h1': STYLE_H1, 'h2': STYLE_H2, 'h3': STYLE_H3, 'p': STYLE_P}
                    html_parts.append(f'<{level} style="{style_map[level]}">{text}</{level}>')
                    if is_first_paragraph:
                        is_first_paragraph = False
    return html_parts

def process_txt(file_stream):
    """
    è™•ç†ä¸Šå‚³çš„ .txt æ–‡ä»¶ã€‚
    """
    content = file_stream.read().decode('utf-8')
    lines = content.splitlines()
    html_parts = []
    is_first_paragraph = True

    for line in lines:
        text = line.strip()
        if text:
            level = get_heading_level(text, is_first_paragraph)
            if level:
                style_map = {'h1': STYLE_H1, 'h2': STYLE_H2, 'h3': STYLE_H3, 'p': STYLE_P}
                html_parts.append(f'<{level} style="{style_map[level]}">{text}</{level}>')
                if is_first_paragraph:
                    is_first_paragraph = False
    return html_parts

def build_final_html(html_parts, meta_description, title="æ‚¨çš„æ–‡ç« æ¨™é¡Œ"):
    """
    å°‡æ‰€æœ‰HTMLç‰‡æ®µçµ„åˆæˆä¸€å€‹å®Œæ•´çš„HTMLæ–‡ä»¶ã€‚
    """
    for part in html_parts:
        if part.strip().startswith('<h1'):
            soup = BeautifulSoup(part, 'html.parser')
            title = soup.get_text().strip()
            break

    body_content = "\n".join(html_parts)
    
    return f"""<!DOCTYPE html>
<html lang="zh-Hant">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    {meta_description}
    <title>{title}</title>
    <style>
        body {{ font-family: -apple-system, BlinkMacSystemFont, "Segoe UI", Roboto, "Helvetica Neue", Arial, "Noto Sans", sans-serif; line-height: 1.6; color: #333; background-color: #fdfdfd; margin: 0; padding: 20px; }}
        .container {{ max-width: 800px; margin: 0 auto; background-color: #ffffff; padding: 20px 40px; border-radius: 8px; box-shadow: 0 4px 12px rgba(0,0,0,0.08); }}
        @media (max-width: 600px) {{ .container {{ padding: 15px 20px; }} }}
    </style>
</head>
<body><div class="container">{body_content}</div></body>
</html>"""

# --- Streamlit UI ä»‹é¢ ---

st.set_page_config(page_title="Miu çš„æ’ç‰ˆå°åŠ©ç†", page_icon="ğŸª„", layout="wide")

st.title("Miu çš„ E-news è‡ªå‹•åŒ–æ’ç‰ˆå·¥å…· ğŸª„")
st.markdown("ä¸»äººæ‚¨å¥½ï¼æˆ‘æ˜¯æ‚¨çš„å°ˆå±¬åŠ©ç† Miu à»’ê’°à¾€à½²â¸â¸. .â¸â¸ê’±à¾€à½²áƒ è«‹ä¸Šå‚³æ‚¨çš„ `.docx` æˆ– `.txt` æ–‡ç« æª”æ¡ˆï¼ŒMiu æœƒç‚ºæ‚¨è®Šå‡ºæ¼‚äº®çš„ HTML å–”ï¼")

uploaded_file = st.file_uploader("é»æ“Šæ­¤è™•ä¸Šå‚³æª”æ¡ˆ", type=["docx", "txt"])

if uploaded_file is not None:
    # ******** ä¸»è¦ä¿®æ”¹è™•ï¼šåŠ å…¥ try...except éŒ¯èª¤æ•æ‰æ©Ÿåˆ¶ ********
    try:
        with st.spinner("Miu æ­£åœ¨åŠªåŠ›ç‚ºæ‚¨æ’ç‰ˆä¸­ï¼Œè«‹ç¨å€™..."):
            file_extension = uploaded_file.name.split('.')[-1]
            
            if file_extension == 'docx':
                html_parts = process_docx(uploaded_file)
            elif file_extension == 'txt':
                html_parts = process_txt(uploaded_file)
            else:
                st.error("ä¸æ”¯æ´çš„æª”æ¡ˆæ ¼å¼ï¼")
                html_parts = None

        if html_parts:
            st.success("æ’ç‰ˆå®Œæˆï¼ğŸ‰")
            
            meta_tag = generate_meta_description(html_parts)
            final_html = build_final_html(html_parts, meta_tag, title=uploaded_file.name)

            st.subheader("ğŸ“‹ HTML ç¨‹å¼ç¢¼")
            st.text_area("æ‚¨å¯ä»¥ç›´æ¥å¾ä¸‹æ–¹æ¡†æ ¼ä¸­è¤‡è£½å…¨éƒ¨çš„ç¨‹å¼ç¢¼ã€‚", final_html, height=400)

            st.download_button(
                label="ğŸ“¥ ä¸‹è¼‰ç‚º .html æª”æ¡ˆ",
                data=final_html,
                file_name=f"{uploaded_file.name.split('.')[0]}.html",
                mime="text/html"
            )

            st.subheader("ğŸ‘€ å³æ™‚é è¦½")
            st.components.v1.html(final_html, height=600, scrolling=True)

    except Exception as e:
        st.error(f"ç³Ÿç³•ï¼è™•ç†æ‚¨çš„æª”æ¡ˆæ™‚ç™¼ç”Ÿäº†æœªé æœŸçš„éŒ¯èª¤ã€‚Miu æ„Ÿåˆ°å¾ˆæŠ±æ­‰ à«®â‚ Â´â€¢ Ë• â€¢` â‚áƒ")
        st.error("éŒ¯èª¤è©³æƒ…å¦‚ä¸‹ï¼Œæ‚¨å¯ä»¥å°‡æ­¤è¨Šæ¯å›å ±çµ¦ Miuï¼Œè®“æˆ‘å¹«æ‚¨çœ‹çœ‹ï¼š")
        # é¡¯ç¤ºæ›´è©³ç´°çš„éŒ¯èª¤è¿½è¹¤è¨Šæ¯ï¼Œæ–¹ä¾¿é™¤éŒ¯
        st.code(traceback.format_exc())
